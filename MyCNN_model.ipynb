{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f3eafc",
   "metadata": {},
   "source": [
    "### Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc833149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ATeX  # Ensure dataloader.py is in the same directory or in PYTHONPATH\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60876e45",
   "metadata": {},
   "source": [
    "### Dataset Preparation with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14da60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations with augmentation\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_std[0], mean_std[1]),\n",
    "])\n",
    "\n",
    "# Define datasets\n",
    "train_dataset = ATeX(split=\"train\", transform=data_transforms)\n",
    "test_dataset = ATeX(split=\"test\", transform=data_transforms)\n",
    "val_dataset = ATeX(split=\"val\", transform=data_transforms)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225424df",
   "metadata": {},
   "source": [
    "### Updated Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df2279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated Custom CNN Model\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cc234",
   "metadata": {},
   "source": [
    "### Optimizer, Loss, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251a4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer, loss function, and learning rate scheduler\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)  # Changed to SGD with momentum\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065dcb4",
   "metadata": {},
   "source": [
    "### Training, Validation and Their Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb525c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 1.8024, Val Loss: 1.7613, Precision: 0.4859, Recall: 0.4337, F1-Score: 0.4095\n",
      "Epoch [2/30] Train Loss: 1.4418, Val Loss: 1.6226, Precision: 0.5496, Recall: 0.4952, F1-Score: 0.4830\n",
      "Epoch [3/30] Train Loss: 1.2997, Val Loss: 1.3076, Precision: 0.6018, Recall: 0.5607, F1-Score: 0.5619\n",
      "Epoch [4/30] Train Loss: 1.1745, Val Loss: 1.1230, Precision: 0.6589, Recall: 0.6190, F1-Score: 0.6217\n",
      "Epoch [5/30] Train Loss: 1.0956, Val Loss: 1.1122, Precision: 0.6181, Recall: 0.6142, F1-Score: 0.6029\n",
      "Epoch [6/30] Train Loss: 0.9242, Val Loss: 0.8861, Precision: 0.7045, Recall: 0.7005, F1-Score: 0.6954\n",
      "Epoch [7/30] Train Loss: 0.8880, Val Loss: 0.9229, Precision: 0.7008, Recall: 0.6845, F1-Score: 0.6828\n",
      "Epoch [8/30] Train Loss: 0.8270, Val Loss: 0.8420, Precision: 0.7085, Recall: 0.6933, F1-Score: 0.6911\n",
      "Epoch [9/30] Train Loss: 0.8135, Val Loss: 0.8715, Precision: 0.7141, Recall: 0.7061, F1-Score: 0.7021\n",
      "Epoch [10/30] Train Loss: 0.7784, Val Loss: 0.8698, Precision: 0.7105, Recall: 0.6925, F1-Score: 0.6909\n",
      "Epoch [11/30] Train Loss: 0.6409, Val Loss: 0.7410, Precision: 0.7621, Recall: 0.7492, F1-Score: 0.7507\n",
      "Epoch [12/30] Train Loss: 0.6269, Val Loss: 0.7148, Precision: 0.7660, Recall: 0.7556, F1-Score: 0.7561\n",
      "Epoch [13/30] Train Loss: 0.6070, Val Loss: 0.6758, Precision: 0.7771, Recall: 0.7756, F1-Score: 0.7728\n",
      "Epoch [14/30] Train Loss: 0.5875, Val Loss: 0.7054, Precision: 0.7642, Recall: 0.7628, F1-Score: 0.7602\n",
      "Epoch [15/30] Train Loss: 0.5527, Val Loss: 0.7014, Precision: 0.7653, Recall: 0.7524, F1-Score: 0.7521\n",
      "Epoch [16/30] Train Loss: 0.5118, Val Loss: 0.6639, Precision: 0.7743, Recall: 0.7716, F1-Score: 0.7687\n",
      "Epoch [17/30] Train Loss: 0.4850, Val Loss: 0.5923, Precision: 0.7975, Recall: 0.7971, F1-Score: 0.7966\n",
      "Epoch [18/30] Train Loss: 0.4692, Val Loss: 0.6157, Precision: 0.7933, Recall: 0.7923, F1-Score: 0.7905\n",
      "Epoch [19/30] Train Loss: 0.4568, Val Loss: 0.6162, Precision: 0.7897, Recall: 0.7875, F1-Score: 0.7852\n",
      "Epoch [20/30] Train Loss: 0.4516, Val Loss: 0.5905, Precision: 0.8002, Recall: 0.7979, F1-Score: 0.7975\n",
      "Epoch [21/30] Train Loss: 0.4002, Val Loss: 0.5814, Precision: 0.8060, Recall: 0.8067, F1-Score: 0.8047\n",
      "Epoch [22/30] Train Loss: 0.3987, Val Loss: 0.5508, Precision: 0.8081, Recall: 0.8067, F1-Score: 0.8059\n",
      "Epoch [23/30] Train Loss: 0.3823, Val Loss: 0.5701, Precision: 0.8090, Recall: 0.8083, F1-Score: 0.8072\n",
      "Epoch [24/30] Train Loss: 0.3802, Val Loss: 0.5793, Precision: 0.8016, Recall: 0.7979, F1-Score: 0.7974\n",
      "Epoch [25/30] Train Loss: 0.3799, Val Loss: 0.5611, Precision: 0.8116, Recall: 0.8075, F1-Score: 0.8075\n",
      "Epoch [26/30] Train Loss: 0.3452, Val Loss: 0.5524, Precision: 0.8196, Recall: 0.8195, F1-Score: 0.8187\n",
      "Epoch [27/30] Train Loss: 0.3480, Val Loss: 0.5491, Precision: 0.8124, Recall: 0.8115, F1-Score: 0.8104\n",
      "Epoch [28/30] Train Loss: 0.3363, Val Loss: 0.5347, Precision: 0.8209, Recall: 0.8187, F1-Score: 0.8179\n",
      "Epoch [29/30] Train Loss: 0.3441, Val Loss: 0.5680, Precision: 0.8141, Recall: 0.8091, F1-Score: 0.8085\n",
      "Epoch [30/30] Train Loss: 0.3368, Val Loss: 0.5724, Precision: 0.8159, Recall: 0.8131, F1-Score: 0.8128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop with learning rate scheduler\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_labels, all_val_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_report = classification_report(\n",
    "        all_val_labels, all_val_preds, output_dict=True, zero_division=0)\n",
    "    val_precision = val_report['weighted avg']['precision']\n",
    "    val_recall = val_report['weighted avg']['recall']\n",
    "    val_f1 = val_report['weighted avg']['f1-score']\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, \"\n",
    "          f\"Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e716d1",
   "metadata": {},
   "source": [
    "### Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3bb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       delta       0.85      0.93      0.89       330\n",
      "     estuary       0.87      0.83      0.85       125\n",
      "       flood       0.79      0.86      0.82       283\n",
      "    glaciers       0.93      0.93      0.93       240\n",
      "  hot_spring       0.75      0.84      0.79       201\n",
      "        lake       0.82      0.75      0.78       153\n",
      "        pool       0.89      0.84      0.86        79\n",
      "      puddle       0.74      0.61      0.67       168\n",
      "      rapids       0.86      0.81      0.83       219\n",
      "       river       0.74      0.63      0.68        73\n",
      "         sea       0.80      0.90      0.85       108\n",
      "        snow       0.85      0.82      0.84       142\n",
      "       swamp       0.90      0.91      0.90       188\n",
      "   waterfall       0.76      0.73      0.74        96\n",
      "     wetland       0.98      0.91      0.94        93\n",
      "\n",
      "    accuracy                           0.84      2498\n",
      "   macro avg       0.84      0.82      0.83      2498\n",
      "weighted avg       0.84      0.84      0.84      2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "all_labels, all_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "report = classification_report(all_labels, all_preds, target_names=train_dataset.classes)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc0b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification report\n",
    "with open(\"classification_report_my_modelwithSGD.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"my_model_cpu.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
