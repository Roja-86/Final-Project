{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f3eafc",
   "metadata": {},
   "source": [
    "### Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc833149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ATeX  # Ensure dataloader.py is in the same directory or in PYTHONPATH\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60876e45",
   "metadata": {},
   "source": [
    "### Dataset Preparation with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14da60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define transformations with augmentation\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_std[0], mean_std[1]),\n",
    "])\n",
    "\n",
    "# Define datasets\n",
    "train_dataset = ATeX(split=\"train\", transform=data_transforms)\n",
    "test_dataset = ATeX(split=\"test\", transform=data_transforms)\n",
    "val_dataset = ATeX(split=\"val\", transform=data_transforms)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225424df",
   "metadata": {},
   "source": [
    "### Updated Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f823078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Fourth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)  # Adjust size based on input size\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 15)  # Output layer (adjust for your class count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Device configuration and model initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cc234",
   "metadata": {},
   "source": [
    "### Optimizer, Loss, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251a4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer, loss function, and learning rate scheduler\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)  # Changed to SGD with momentum\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065dcb4",
   "metadata": {},
   "source": [
    "### Training, Validation and Their Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3745c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] Train Loss: 1.8066, Val Loss: 1.6728, Precision: 0.5412, Recall: 0.4848, F1-Score: 0.4937\n",
      "Epoch [2/30] Train Loss: 1.4806, Val Loss: 1.5793, Precision: 0.5711, Recall: 0.5216, F1-Score: 0.5094\n",
      "Epoch [3/30] Train Loss: 1.3894, Val Loss: 1.3213, Precision: 0.5868, Recall: 0.5479, F1-Score: 0.5415\n",
      "Epoch [4/30] Train Loss: 1.2193, Val Loss: 1.5107, Precision: 0.5991, Recall: 0.5312, F1-Score: 0.5262\n",
      "Epoch [5/30] Train Loss: 1.1504, Val Loss: 1.2186, Precision: 0.6174, Recall: 0.5831, F1-Score: 0.5806\n",
      "Epoch [6/30] Train Loss: 0.9893, Val Loss: 0.9723, Precision: 0.6657, Recall: 0.6597, F1-Score: 0.6516\n",
      "Epoch [7/30] Train Loss: 0.9121, Val Loss: 0.9389, Precision: 0.7001, Recall: 0.6861, F1-Score: 0.6856\n",
      "Epoch [8/30] Train Loss: 0.9042, Val Loss: 1.0237, Precision: 0.6860, Recall: 0.6629, F1-Score: 0.6605\n",
      "Epoch [9/30] Train Loss: 0.8454, Val Loss: 0.8619, Precision: 0.7127, Recall: 0.7029, F1-Score: 0.6947\n",
      "Epoch [10/30] Train Loss: 0.8236, Val Loss: 0.9872, Precision: 0.6865, Recall: 0.6621, F1-Score: 0.6639\n",
      "Epoch [11/30] Train Loss: 0.7083, Val Loss: 0.7458, Precision: 0.7633, Recall: 0.7588, F1-Score: 0.7591\n",
      "Epoch [12/30] Train Loss: 0.6695, Val Loss: 0.8200, Precision: 0.7373, Recall: 0.7292, F1-Score: 0.7257\n",
      "Epoch [13/30] Train Loss: 0.6539, Val Loss: 0.7242, Precision: 0.7633, Recall: 0.7524, F1-Score: 0.7524\n",
      "Epoch [14/30] Train Loss: 0.6135, Val Loss: 0.7560, Precision: 0.7511, Recall: 0.7380, F1-Score: 0.7398\n",
      "Epoch [15/30] Train Loss: 0.6119, Val Loss: 0.6850, Precision: 0.7703, Recall: 0.7676, F1-Score: 0.7664\n",
      "Epoch [16/30] Train Loss: 0.5310, Val Loss: 0.7669, Precision: 0.7394, Recall: 0.7356, F1-Score: 0.7320\n",
      "Epoch [17/30] Train Loss: 0.5215, Val Loss: 0.5863, Precision: 0.8100, Recall: 0.8083, F1-Score: 0.8068\n",
      "Epoch [18/30] Train Loss: 0.4933, Val Loss: 0.6315, Precision: 0.7974, Recall: 0.7891, F1-Score: 0.7884\n",
      "Epoch [19/30] Train Loss: 0.4972, Val Loss: 0.6838, Precision: 0.7624, Recall: 0.7612, F1-Score: 0.7576\n",
      "Epoch [20/30] Train Loss: 0.5014, Val Loss: 0.5764, Precision: 0.8090, Recall: 0.8067, F1-Score: 0.8051\n",
      "Epoch [21/30] Train Loss: 0.4493, Val Loss: 0.5547, Precision: 0.8114, Recall: 0.8067, F1-Score: 0.8056\n",
      "Epoch [22/30] Train Loss: 0.4216, Val Loss: 0.5785, Precision: 0.7992, Recall: 0.7963, F1-Score: 0.7952\n",
      "Epoch [23/30] Train Loss: 0.3979, Val Loss: 0.5419, Precision: 0.8308, Recall: 0.8227, F1-Score: 0.8234\n",
      "Epoch [24/30] Train Loss: 0.4089, Val Loss: 0.5412, Precision: 0.8260, Recall: 0.8227, F1-Score: 0.8213\n",
      "Epoch [25/30] Train Loss: 0.4114, Val Loss: 0.5498, Precision: 0.8259, Recall: 0.8251, F1-Score: 0.8241\n",
      "Epoch [26/30] Train Loss: 0.3820, Val Loss: 0.5461, Precision: 0.8213, Recall: 0.8203, F1-Score: 0.8187\n",
      "Epoch [27/30] Train Loss: 0.3686, Val Loss: 0.5242, Precision: 0.8267, Recall: 0.8267, F1-Score: 0.8255\n",
      "Epoch [28/30] Train Loss: 0.3684, Val Loss: 0.5226, Precision: 0.8324, Recall: 0.8315, F1-Score: 0.8309\n",
      "Epoch [29/30] Train Loss: 0.3652, Val Loss: 0.5317, Precision: 0.8266, Recall: 0.8251, F1-Score: 0.8244\n",
      "Epoch [30/30] Train Loss: 0.3603, Val Loss: 0.5042, Precision: 0.8241, Recall: 0.8219, F1-Score: 0.8210\n",
      "Training metrics saved to: D:/USC_Course/CSCE 790 Section 007 Neural Networks and Their Applications/atex-main/loss_training time/training_metrics.csv\n",
      "Total training time for all epochs: 9.95 minutes\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store metrics\n",
    "metrics = []\n",
    "\n",
    "# Address to save the CSV file\n",
    "save_dir = r\"D:/USC_Course/CSCE 790 Section 007 Neural Networks and Their Applications/atex-main/loss_training time/\"\n",
    "save_path = os.path.join(save_dir, \"training_metrics_custom_model.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Record the start time of the training process\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Training loop with learning rate scheduler\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_labels, all_val_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "    val_loss /= len(val_loader)\n",
    "    val_report = classification_report(\n",
    "        all_val_labels, all_val_preds, output_dict=True, zero_division=0)\n",
    "    val_precision = val_report['weighted avg']['precision']\n",
    "    val_recall = val_report['weighted avg']['recall']\n",
    "    val_f1 = val_report['weighted avg']['f1-score']\n",
    "\n",
    "    # Save metrics for this epoch\n",
    "    metrics.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"precision\": val_precision,\n",
    "        \"recall\": val_recall,\n",
    "        \"f1_score\": val_f1\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, \"\n",
    "          f\"Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "# Record the end time of the training process\n",
    "training_end_time = time.time()\n",
    "total_training_time_seconds = training_end_time - training_start_time\n",
    "total_training_time_minutes = total_training_time_seconds / 60\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "with open(save_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = [\"epoch\", \"train_loss\", \"val_loss\", \"precision\", \"recall\", \"f1_score\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    writer.writerows(metrics)\n",
    "\n",
    "# Save total training time in minutes\n",
    "with open(save_path, 'a', newline='') as csvfile:\n",
    "    csvfile.write(f\"\\nTotal Training Time (minutes):,{total_training_time_minutes:.2f}\\n\")\n",
    "\n",
    "print(f\"Training metrics saved to: {save_path}\")\n",
    "print(f\"Total training time for all epochs: {total_training_time_minutes:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e716d1",
   "metadata": {},
   "source": [
    "### Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3bb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       delta       0.87      0.92      0.89       330\n",
      "     estuary       0.84      0.84      0.84       125\n",
      "       flood       0.81      0.80      0.81       283\n",
      "    glaciers       0.89      0.94      0.92       240\n",
      "  hot_spring       0.76      0.83      0.79       201\n",
      "        lake       0.79      0.82      0.80       153\n",
      "        pool       0.92      0.92      0.92        79\n",
      "      puddle       0.74      0.67      0.70       168\n",
      "      rapids       0.83      0.80      0.82       219\n",
      "       river       0.78      0.67      0.72        73\n",
      "         sea       0.82      0.87      0.84       108\n",
      "        snow       0.85      0.79      0.82       142\n",
      "       swamp       0.91      0.88      0.89       188\n",
      "   waterfall       0.74      0.71      0.72        96\n",
      "     wetland       0.95      0.95      0.95        93\n",
      "\n",
      "    accuracy                           0.84      2498\n",
      "   macro avg       0.83      0.83      0.83      2498\n",
      "weighted avg       0.83      0.84      0.83      2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "all_labels, all_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "report = classification_report(all_labels, all_preds, target_names=train_dataset.classes)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc0b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification report\n",
    "with open(\"classification_report_my_modelwithSGD.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"my_model_cpu.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
