{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import ATeX  # Ensure dataloader.py is in the same directory or in PYTHONPATH\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import classification_report  # Import the function\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "from torchvision.models import shufflenet_v2_x1_0\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation and DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations and dataset\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_std[0], mean_std[1]),\n",
    "])\n",
    "\n",
    "train_dataset = ATeX(split=\"train\", transform=data_transforms)\n",
    "test_dataset = ATeX(split=\"test\", transform=data_transforms)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Validation Dataset and DataLoader\n",
    "val_dataset = ATeX(split=\"val\", transform=data_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ShuffleNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNetV2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage2): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(24, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(24, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=58, bias=False)\n",
       "        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=58, bias=False)\n",
       "        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=58, bias=False)\n",
       "        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=58, bias=False)\n",
       "        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(58, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=116, bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116, bias=False)\n",
       "        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(116, 116, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): InvertedResidual(\n",
       "      (branch1): Sequential(\n",
       "        (0): Conv2d(232, 232, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=232, bias=False)\n",
       "        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "      )\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=232, bias=False)\n",
       "        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232, bias=False)\n",
       "        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232, bias=False)\n",
       "        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (branch1): Sequential()\n",
       "      (branch2): Sequential(\n",
       "        (0): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232, bias=False)\n",
       "        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): Conv2d(232, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (6): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(464, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=1024, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of classes\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize ShuffleNetV2 with pretrained weights\n",
    "model = shufflenet_v2_x1_0(weights=\"DEFAULT\")\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.3),  # Increased dropout for regularization\n",
    "    nn.Linear(model.fc.in_features, num_classes)\n",
    ")\n",
    "\n",
    "# Unfreeze earlier layers for fine-tuning\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" not in name:  # Skip the fully connected layers\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: Train Loss: 2.5495, Val Loss: 2.3648, Precision: 0.2383, Recall: 0.2819, F1-Score: 0.1915\n",
      "Epoch [2/30]: Train Loss: 2.1480, Val Loss: 1.9387, Precision: 0.4151, Recall: 0.4097, F1-Score: 0.3232\n",
      "Epoch [3/30]: Train Loss: 1.7621, Val Loss: 1.5196, Precision: 0.5312, Recall: 0.5407, F1-Score: 0.4787\n",
      "Epoch [4/30]: Train Loss: 1.3244, Val Loss: 1.0757, Precision: 0.6911, Recall: 0.6893, F1-Score: 0.6755\n",
      "Epoch [5/30]: Train Loss: 0.9457, Val Loss: 0.8030, Precision: 0.7621, Recall: 0.7604, F1-Score: 0.7565\n",
      "Epoch [6/30]: Train Loss: 0.7390, Val Loss: 0.7367, Precision: 0.7815, Recall: 0.7748, F1-Score: 0.7732\n",
      "Epoch [7/30]: Train Loss: 0.6219, Val Loss: 0.6238, Precision: 0.8093, Recall: 0.8035, F1-Score: 0.8030\n",
      "Epoch [8/30]: Train Loss: 0.5309, Val Loss: 0.5342, Precision: 0.8396, Recall: 0.8355, F1-Score: 0.8360\n",
      "Epoch [9/30]: Train Loss: 0.4467, Val Loss: 0.5413, Precision: 0.8336, Recall: 0.8299, F1-Score: 0.8302\n",
      "Epoch [10/30]: Train Loss: 0.3936, Val Loss: 0.5114, Precision: 0.8452, Recall: 0.8435, F1-Score: 0.8432\n",
      "Epoch [11/30]: Train Loss: 0.3286, Val Loss: 0.5110, Precision: 0.8496, Recall: 0.8498, F1-Score: 0.8488\n",
      "Epoch [12/30]: Train Loss: 0.2959, Val Loss: 0.4852, Precision: 0.8462, Recall: 0.8450, F1-Score: 0.8448\n",
      "Epoch [13/30]: Train Loss: 0.2386, Val Loss: 0.4630, Precision: 0.8608, Recall: 0.8610, F1-Score: 0.8603\n",
      "Epoch [14/30]: Train Loss: 0.2166, Val Loss: 0.4495, Precision: 0.8634, Recall: 0.8626, F1-Score: 0.8621\n",
      "Epoch [15/30]: Train Loss: 0.2012, Val Loss: 0.4655, Precision: 0.8569, Recall: 0.8554, F1-Score: 0.8553\n",
      "Epoch [16/30]: Train Loss: 0.1771, Val Loss: 0.4735, Precision: 0.8546, Recall: 0.8514, F1-Score: 0.8503\n",
      "Epoch [17/30]: Train Loss: 0.1446, Val Loss: 0.4205, Precision: 0.8744, Recall: 0.8738, F1-Score: 0.8736\n",
      "Epoch [18/30]: Train Loss: 0.1311, Val Loss: 0.3976, Precision: 0.8763, Recall: 0.8770, F1-Score: 0.8761\n",
      "Epoch [19/30]: Train Loss: 0.1231, Val Loss: 0.4068, Precision: 0.8723, Recall: 0.8714, F1-Score: 0.8714\n",
      "Epoch [20/30]: Train Loss: 0.0959, Val Loss: 0.4180, Precision: 0.8744, Recall: 0.8722, F1-Score: 0.8727\n",
      "Epoch [21/30]: Train Loss: 0.0922, Val Loss: 0.4184, Precision: 0.8728, Recall: 0.8706, F1-Score: 0.8707\n",
      "Epoch [22/30]: Train Loss: 0.0758, Val Loss: 0.4051, Precision: 0.8803, Recall: 0.8794, F1-Score: 0.8792\n",
      "Epoch [23/30]: Train Loss: 0.0759, Val Loss: 0.4056, Precision: 0.8837, Recall: 0.8834, F1-Score: 0.8833\n",
      "Epoch [24/30]: Train Loss: 0.0719, Val Loss: 0.4079, Precision: 0.8803, Recall: 0.8802, F1-Score: 0.8800\n",
      "Epoch [25/30]: Train Loss: 0.0661, Val Loss: 0.4022, Precision: 0.8799, Recall: 0.8802, F1-Score: 0.8796\n",
      "Epoch [26/30]: Train Loss: 0.0574, Val Loss: 0.4031, Precision: 0.8829, Recall: 0.8826, F1-Score: 0.8822\n",
      "Epoch [27/30]: Train Loss: 0.0602, Val Loss: 0.4017, Precision: 0.8813, Recall: 0.8810, F1-Score: 0.8806\n",
      "Epoch [28/30]: Train Loss: 0.0531, Val Loss: 0.4111, Precision: 0.8768, Recall: 0.8770, F1-Score: 0.8765\n",
      "Epoch [29/30]: Train Loss: 0.0591, Val Loss: 0.3957, Precision: 0.8805, Recall: 0.8810, F1-Score: 0.8803\n",
      "Epoch [30/30]: Train Loss: 0.0532, Val Loss: 0.4046, Precision: 0.8794, Recall: 0.8794, F1-Score: 0.8788\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30)  # Cosine annealing over 30 epochs\n",
    "# Define the loss function (CrossEntropyLoss or Label Smoothing)\n",
    "criterion = nn.CrossEntropyLoss()  # Replace with label_smoothing_loss if needed\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  # Use the defined loss function\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    val_report = classification_report(\n",
    "        all_val_labels, all_val_preds, target_names=train_dataset.classes, zero_division=0, output_dict=True\n",
    "    )\n",
    "    val_precision = val_report['weighted avg']['precision']\n",
    "    val_recall = val_report['weighted avg']['recall']\n",
    "    val_f1 = val_report['weighted avg']['f1-score']\n",
    "\n",
    "    # Print training and validation metrics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]: Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, \"\n",
    "          f\"Recall: {val_recall:.4f}, F1-Score: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       delta       0.95      0.95      0.95       330\n",
      "     estuary       0.87      0.90      0.88       125\n",
      "       flood       0.85      0.88      0.86       283\n",
      "    glaciers       0.97      0.93      0.95       240\n",
      "  hot_spring       0.86      0.87      0.86       201\n",
      "        lake       0.88      0.85      0.86       153\n",
      "        pool       0.95      0.90      0.92        79\n",
      "      puddle       0.81      0.77      0.79       168\n",
      "      rapids       0.83      0.84      0.83       219\n",
      "       river       0.72      0.66      0.69        73\n",
      "         sea       0.81      0.90      0.85       108\n",
      "        snow       0.88      0.92      0.90       142\n",
      "       swamp       0.94      0.95      0.95       188\n",
      "   waterfall       0.85      0.79      0.82        96\n",
      "     wetland       0.91      0.98      0.94        93\n",
      "\n",
      "    accuracy                           0.88      2498\n",
      "   macro avg       0.87      0.87      0.87      2498\n",
      "weighted avg       0.88      0.88      0.88      2498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, _ in test_loader:  # Assuming your DataLoader provides 3 items\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate classification metrics\n",
    "class_names = train_dataset.classes  # Assuming train_dataset has the class names\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Compute per-class accuracy\n",
    "all_labels = np.array(all_labels)\n",
    "all_preds = np.array(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification report\n",
    "with open(\"classification_report_shufflenetv2_model.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"shufflenetv2_model_cpu.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
